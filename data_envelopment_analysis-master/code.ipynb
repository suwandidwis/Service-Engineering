{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.optimize import fmin_slsqp\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "from pprint import pprint\n",
    "import random\n",
    "\n",
    "#Listing the existing files in the dir\n",
    "# for root, dirs, files in os.walk(\".\"):  \n",
    "#     for filename in files:\n",
    "#         print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading df\n",
    "df = pd.read_csv('./files/data-2018-09-24.csv')\n",
    "df = df.drop(df.loc[:,['name', 'pft']].head(0).columns, axis=1)\n",
    "inpt_df = df.iloc[:, 1:6]\n",
    "inpt_arr = np.array(inpt_df)\n",
    "outpt_df = df.iloc[:, 6:]\n",
    "outpt_arr = np.array(outpt_df)\n",
    "comp = np.array(df.iloc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x9396240>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#For the article\n",
    "inputs = pd.DataFrame(inpt_df.iloc[:4,4])\n",
    "outputs = outpt_df.iloc[:4,0:2]\n",
    "l = []\n",
    "for col in inputs.columns:\n",
    "    l.append(outputs.div(inputs[col], axis=0))\n",
    "complete = pd.concat(l, axis=1)\n",
    "complete.columns = ['y1', 'y2']\n",
    "completed = pd.DataFrame(df.iloc[:4, 0]).join(complete)\n",
    "\n",
    "#Plotting the graph\n",
    "p1 = sns.regplot(data = completed, x=\"y1\", y=\"y2\", fit_reg = False, marker=\"o\", color = 'black')\n",
    " \n",
    "# add annotations one by one with a loop\n",
    "for line in range(0,completed.shape[0]):\n",
    "     p1.text(completed.y1[line]+0.2, completed.y2[line], completed.firm[line], horizontalalignment='left', size='medium', color='black', weight='semibold')\n",
    "\n",
    "p1.set_title('Efficient frontier')\n",
    "\n",
    "plt.plot([0, 4.938],[6.778,6.778], \"r--\")\n",
    "plt.plot([4.938, 6.058889],[6.778, 6.553333], \"r--\")\n",
    "plt.plot([6.058889, 6.058889],[6.553333, 0], \"r--\")\n",
    "plt.plot([0,5.8],[0,6.6] , \"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>assists</th>\n",
       "      <th>games</th>\n",
       "      <th>goals</th>\n",
       "      <th>minutes</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>39</td>\n",
       "      <td>12</td>\n",
       "      <td>2774</td>\n",
       "      <td>Leroy Sané</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>36</td>\n",
       "      <td>20</td>\n",
       "      <td>2774</td>\n",
       "      <td>Raheem Sterling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>44</td>\n",
       "      <td>7</td>\n",
       "      <td>2329</td>\n",
       "      <td>Bernardo Silva</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>37</td>\n",
       "      <td>30</td>\n",
       "      <td>2895</td>\n",
       "      <td>Sergio Agüero</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>11</td>\n",
       "      <td>1766</td>\n",
       "      <td>Gabriel Jesus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   assists  games  goals  minutes             name\n",
       "0       15     39     12     2774       Leroy Sané\n",
       "1       10     36     20     2774  Raheem Sterling\n",
       "2       11     44      7     2329   Bernardo Silva\n",
       "3        7     37     30     2895    Sergio Agüero\n",
       "4        4     31     11     1766    Gabriel Jesus"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading df\n",
    "df2 = pd.read_csv('StrikerPerformance.csv') #skiprows = 1)\n",
    "df2 = df2.drop(df2.loc[:,['age', 'current club', 'current league', 'foot', 'height', 'nationality', 'position']].head(0).columns, axis=1)\n",
    "df2.head() #where xs are input vars and ys are output vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df2 = pd.read_csv('StrikerPerformance.csv')\n",
    "inputss = pd.DataFrame(df2.iloc[:,[1,3]])\n",
    "outputss = pd.DataFrame(df2.iloc[:,[0,2]])\n",
    "comp_2 = np.array(df2.iloc[:, 4])\n",
    "inpt_arr_2 = np.array(inputss)\n",
    "outpt_arr_2 = np.array(outputss)\n",
    "l = range(inpt_arr_2.shape[1])\n",
    "np.zeros((inpt_arr_2.shape[1],1), dtype=np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.0000000000436755\n",
      "            Iterations: 17\n",
      "            Function evaluations: 2264\n",
      "            Gradient evaluations: 17\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.8860157504907372\n",
      "            Iterations: 14\n",
      "            Function evaluations: 1849\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.5900071914830856\n",
      "            Iterations: 6\n",
      "            Function evaluations: 802\n",
      "            Gradient evaluations: 6\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.990629679735921\n",
      "            Iterations: 15\n",
      "            Function evaluations: 1979\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.6185686543520579\n",
      "            Iterations: 9\n",
      "            Function evaluations: 1191\n",
      "            Gradient evaluations: 9\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.5698035272020328\n",
      "            Iterations: 7\n",
      "            Function evaluations: 924\n",
      "            Gradient evaluations: 7\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.8356263213416812\n",
      "            Iterations: 12\n",
      "            Function evaluations: 1576\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.6821794211479711\n",
      "            Iterations: 14\n",
      "            Function evaluations: 1717\n",
      "            Gradient evaluations: 13\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 3.552930927783648\n",
      "            Iterations: 7\n",
      "            Function evaluations: 940\n",
      "            Gradient evaluations: 7\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.6826264737515373\n",
      "            Iterations: 12\n",
      "            Function evaluations: 1593\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.4922843443884874\n",
      "            Iterations: 12\n",
      "            Function evaluations: 1584\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.7231103038655945\n",
      "            Iterations: 9\n",
      "            Function evaluations: 1183\n",
      "            Gradient evaluations: 9\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 5.199148905236307\n",
      "            Iterations: 8\n",
      "            Function evaluations: 1077\n",
      "            Gradient evaluations: 8\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.6776197089005593\n",
      "            Iterations: 12\n",
      "            Function evaluations: 1455\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.0000000001497893\n",
      "            Iterations: 10\n",
      "            Function evaluations: 1189\n",
      "            Gradient evaluations: 9\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.4933603121682915\n",
      "            Iterations: 11\n",
      "            Function evaluations: 1455\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.8037364904016352\n",
      "            Iterations: 13\n",
      "            Function evaluations: 1583\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.40456154203138417\n",
      "            Iterations: 13\n",
      "            Function evaluations: 1723\n",
      "            Gradient evaluations: 13\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.7355765626354404\n",
      "            Iterations: 15\n",
      "            Function evaluations: 1855\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.5385675970074294\n",
      "            Iterations: 14\n",
      "            Function evaluations: 1849\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.5282342968346834\n",
      "            Iterations: 10\n",
      "            Function evaluations: 1327\n",
      "            Gradient evaluations: 10\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.6977375189597136\n",
      "            Iterations: 11\n",
      "            Function evaluations: 1319\n",
      "            Gradient evaluations: 10\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.6745689693820012\n",
      "            Iterations: 13\n",
      "            Function evaluations: 1727\n",
      "            Gradient evaluations: 13\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.0000002575762057\n",
      "            Iterations: 16\n",
      "            Function evaluations: 1983\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 7.891175207681022\n",
      "            Iterations: 6\n",
      "            Function evaluations: 806\n",
      "            Gradient evaluations: 6\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.15304853737529286\n",
      "            Iterations: 9\n",
      "            Function evaluations: 1190\n",
      "            Gradient evaluations: 9\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.4437275756501528\n",
      "            Iterations: 11\n",
      "            Function evaluations: 1461\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.9162127014998505\n",
      "            Iterations: 16\n",
      "            Function evaluations: 2109\n",
      "            Gradient evaluations: 16\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.4779646855101876\n",
      "            Iterations: 12\n",
      "            Function evaluations: 1454\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.29466293709178976\n",
      "            Iterations: 12\n",
      "            Function evaluations: 1585\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: -0.0\n",
      "            Iterations: 3\n",
      "            Function evaluations: 394\n",
      "            Gradient evaluations: 3\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.737531212331295\n",
      "            Iterations: 16\n",
      "            Function evaluations: 1981\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.7240181018714383\n",
      "            Iterations: 13\n",
      "            Function evaluations: 1715\n",
      "            Gradient evaluations: 13\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.4811589777566421\n",
      "            Iterations: 9\n",
      "            Function evaluations: 1193\n",
      "            Gradient evaluations: 9\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.9534418594150132\n",
      "            Iterations: 14\n",
      "            Function evaluations: 1846\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.765327085457235\n",
      "            Iterations: 8\n",
      "            Function evaluations: 1083\n",
      "            Gradient evaluations: 8\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.2769748548119356\n",
      "            Iterations: 10\n",
      "            Function evaluations: 1322\n",
      "            Gradient evaluations: 10\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 13.401591215005622\n",
      "            Iterations: 8\n",
      "            Function evaluations: 1077\n",
      "            Gradient evaluations: 8\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.3540659940349833\n",
      "            Iterations: 12\n",
      "            Function evaluations: 1586\n",
      "            Gradient evaluations: 12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.81828577396445\n",
      "            Iterations: 9\n",
      "            Function evaluations: 1193\n",
      "            Gradient evaluations: 9\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.23049124135648472\n",
      "            Iterations: 14\n",
      "            Function evaluations: 1860\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.6206030540367182\n",
      "            Iterations: 13\n",
      "            Function evaluations: 1710\n",
      "            Gradient evaluations: 13\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: -4.2666053255789286e-05\n",
      "            Iterations: 30\n",
      "            Function evaluations: 3480\n",
      "            Gradient evaluations: 26\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.20812101894422477\n",
      "            Iterations: 12\n",
      "            Function evaluations: 1581\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.3764707339982663\n",
      "            Iterations: 14\n",
      "            Function evaluations: 1715\n",
      "            Gradient evaluations: 13\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.2580974615173097\n",
      "            Iterations: 6\n",
      "            Function evaluations: 786\n",
      "            Gradient evaluations: 6\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.39577796697242795\n",
      "            Iterations: 12\n",
      "            Function evaluations: 1578\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.20807359333228428\n",
      "            Iterations: 13\n",
      "            Function evaluations: 1719\n",
      "            Gradient evaluations: 13\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.124052421633353\n",
      "            Iterations: 9\n",
      "            Function evaluations: 1216\n",
      "            Gradient evaluations: 9\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.6995839538863995\n",
      "            Iterations: 10\n",
      "            Function evaluations: 1356\n",
      "            Gradient evaluations: 10\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.6966683646375678\n",
      "            Iterations: 10\n",
      "            Function evaluations: 1321\n",
      "            Gradient evaluations: 10\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.5086362638475742\n",
      "            Iterations: 13\n",
      "            Function evaluations: 1719\n",
      "            Gradient evaluations: 13\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.2192244820209358\n",
      "            Iterations: 11\n",
      "            Function evaluations: 1455\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.7692797524553168\n",
      "            Iterations: 15\n",
      "            Function evaluations: 1856\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.40886805683108346\n",
      "            Iterations: 9\n",
      "            Function evaluations: 1201\n",
      "            Gradient evaluations: 9\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.5870967753012742\n",
      "            Iterations: 16\n",
      "            Function evaluations: 2122\n",
      "            Gradient evaluations: 16\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: -0.0\n",
      "            Iterations: 2\n",
      "            Function evaluations: 263\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 3.0413299044330437\n",
      "            Iterations: 7\n",
      "            Function evaluations: 949\n",
      "            Gradient evaluations: 7\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.9241573262360382\n",
      "            Iterations: 12\n",
      "            Function evaluations: 1594\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.4333333425781364\n",
      "            Iterations: 12\n",
      "            Function evaluations: 1582\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.4893516372985351\n",
      "            Iterations: 9\n",
      "            Function evaluations: 1192\n",
      "            Gradient evaluations: 9\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.30759318003571934\n",
      "            Iterations: 10\n",
      "            Function evaluations: 1324\n",
      "            Gradient evaluations: 10\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.5752097972787827\n",
      "            Iterations: 13\n",
      "            Function evaluations: 1733\n",
      "            Gradient evaluations: 13\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.1831441253371344\n",
      "            Iterations: 8\n",
      "            Function evaluations: 1055\n",
      "            Gradient evaluations: 8\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.24021120443175992\n",
      "            Iterations: 10\n",
      "            Function evaluations: 1327\n",
      "            Gradient evaluations: 10\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.3198418106223112\n",
      "            Iterations: 11\n",
      "            Function evaluations: 1450\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.40882967608244974\n",
      "            Iterations: 10\n",
      "            Function evaluations: 1197\n",
      "            Gradient evaluations: 9\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.5408123748219424\n",
      "            Iterations: 11\n",
      "            Function evaluations: 1328\n",
      "            Gradient evaluations: 10\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.21109246800093356\n",
      "            Iterations: 11\n",
      "            Function evaluations: 1328\n",
      "            Gradient evaluations: 10\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.5098800089061011\n",
      "            Iterations: 15\n",
      "            Function evaluations: 1864\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.36572421453886966\n",
      "            Iterations: 8\n",
      "            Function evaluations: 1056\n",
      "            Gradient evaluations: 8\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.4501642932429256\n",
      "            Iterations: 8\n",
      "            Function evaluations: 1059\n",
      "            Gradient evaluations: 8\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.3187652145800036\n",
      "            Iterations: 8\n",
      "            Function evaluations: 1057\n",
      "            Gradient evaluations: 8\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: -0.0\n",
      "            Iterations: 1\n",
      "            Function evaluations: 132\n",
      "            Gradient evaluations: 1\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.2960038947042017\n",
      "            Iterations: 10\n",
      "            Function evaluations: 1317\n",
      "            Gradient evaluations: 10\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0795654427404867\n",
      "            Iterations: 12\n",
      "            Function evaluations: 1585\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.18356375048452236\n",
      "            Iterations: 8\n",
      "            Function evaluations: 1052\n",
      "            Gradient evaluations: 8\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.3679279100676915\n",
      "            Iterations: 11\n",
      "            Function evaluations: 1455\n",
      "            Gradient evaluations: 11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.29293809860066944\n",
      "            Iterations: 13\n",
      "            Function evaluations: 1729\n",
      "            Gradient evaluations: 13\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.4944063001361981\n",
      "            Iterations: 7\n",
      "            Function evaluations: 942\n",
      "            Gradient evaluations: 7\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 9.464699037349193\n",
      "            Iterations: 6\n",
      "            Function evaluations: 804\n",
      "            Gradient evaluations: 6\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.3221144333565683\n",
      "            Iterations: 13\n",
      "            Function evaluations: 1589\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.3008206493529987\n",
      "            Iterations: 14\n",
      "            Function evaluations: 1847\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.4679449240996936\n",
      "            Iterations: 16\n",
      "            Function evaluations: 1978\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.1842996341361579\n",
      "            Iterations: 13\n",
      "            Function evaluations: 1719\n",
      "            Gradient evaluations: 13\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.7891016584380778\n",
      "            Iterations: 9\n",
      "            Function evaluations: 1189\n",
      "            Gradient evaluations: 9\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.556122877176301\n",
      "            Iterations: 12\n",
      "            Function evaluations: 1587\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.20187975552954504\n",
      "            Iterations: 12\n",
      "            Function evaluations: 1588\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.3375147309039379\n",
      "            Iterations: 14\n",
      "            Function evaluations: 1849\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.5107656175520306\n",
      "            Iterations: 9\n",
      "            Function evaluations: 1179\n",
      "            Gradient evaluations: 9\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.3562781196489725\n",
      "            Iterations: 12\n",
      "            Function evaluations: 1582\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.21054405064160112\n",
      "            Iterations: 18\n",
      "            Function evaluations: 2387\n",
      "            Gradient evaluations: 18\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.1407137943526543\n",
      "            Iterations: 11\n",
      "            Function evaluations: 1454\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.7774726123155066\n",
      "            Iterations: 12\n",
      "            Function evaluations: 1613\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.42631366172320306\n",
      "            Iterations: 15\n",
      "            Function evaluations: 1847\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.24052399445174655\n",
      "            Iterations: 11\n",
      "            Function evaluations: 1448\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.4745213703891534\n",
      "            Iterations: 13\n",
      "            Function evaluations: 1724\n",
      "            Gradient evaluations: 13\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.3961349349207346\n",
      "            Iterations: 7\n",
      "            Function evaluations: 943\n",
      "            Gradient evaluations: 7\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.48750025099337885\n",
      "            Iterations: 9\n",
      "            Function evaluations: 1205\n",
      "            Gradient evaluations: 9\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0\n",
      "            Iterations: 1\n",
      "            Function evaluations: 132\n",
      "            Gradient evaluations: 1\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.5835336445848287\n",
      "            Iterations: 8\n",
      "            Function evaluations: 1059\n",
      "            Gradient evaluations: 8\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 3.976400816548532\n",
      "            Iterations: 8\n",
      "            Function evaluations: 1077\n",
      "            Gradient evaluations: 8\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.0\n",
      "            Iterations: 1\n",
      "            Function evaluations: 132\n",
      "            Gradient evaluations: 1\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.554143095373424\n",
      "            Iterations: 7\n",
      "            Function evaluations: 938\n",
      "            Gradient evaluations: 7\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: -0.0\n",
      "            Iterations: 2\n",
      "            Function evaluations: 264\n",
      "            Gradient evaluations: 2\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.29685636607463417\n",
      "            Iterations: 7\n",
      "            Function evaluations: 943\n",
      "            Gradient evaluations: 7\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.9930215783444133\n",
      "            Iterations: 12\n",
      "            Function evaluations: 1584\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.49426720609816516\n",
      "            Iterations: 15\n",
      "            Function evaluations: 1981\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.41304522016563033\n",
      "            Iterations: 5\n",
      "            Function evaluations: 656\n",
      "            Gradient evaluations: 5\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.6807413347646241\n",
      "            Iterations: 11\n",
      "            Function evaluations: 1455\n",
      "            Gradient evaluations: 11\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 5.540486312576086\n",
      "            Iterations: 9\n",
      "            Function evaluations: 1207\n",
      "            Gradient evaluations: 9\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.36310344838049896\n",
      "            Iterations: 10\n",
      "            Function evaluations: 1323\n",
      "            Gradient evaluations: 10\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.3747572960234425\n",
      "            Iterations: 13\n",
      "            Function evaluations: 1590\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.2660613269830555\n",
      "            Iterations: 10\n",
      "            Function evaluations: 1323\n",
      "            Gradient evaluations: 10\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.7023065165694732\n",
      "            Iterations: 17\n",
      "            Function evaluations: 2122\n",
      "            Gradient evaluations: 16\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.496397723274667\n",
      "            Iterations: 9\n",
      "            Function evaluations: 1192\n",
      "            Gradient evaluations: 9\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.25538441103723863\n",
      "            Iterations: 10\n",
      "            Function evaluations: 1322\n",
      "            Gradient evaluations: 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.2963212825757345\n",
      "            Iterations: 13\n",
      "            Function evaluations: 1717\n",
      "            Gradient evaluations: 13\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.07352806951116084\n",
      "            Iterations: 9\n",
      "            Function evaluations: 1192\n",
      "            Gradient evaluations: 9\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.22799767567824783\n",
      "            Iterations: 10\n",
      "            Function evaluations: 1327\n",
      "            Gradient evaluations: 10\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.14309778625276393\n",
      "            Iterations: 15\n",
      "            Function evaluations: 1862\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.4600439865400728\n",
      "            Iterations: 12\n",
      "            Function evaluations: 1583\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.2928379752209093\n",
      "            Iterations: 12\n",
      "            Function evaluations: 1587\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.23854622509850237\n",
      "            Iterations: 10\n",
      "            Function evaluations: 1354\n",
      "            Gradient evaluations: 10\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.44375672180611603\n",
      "            Iterations: 18\n",
      "            Function evaluations: 2134\n",
      "            Gradient evaluations: 16\n",
      "---------------------------\n",
      "\n",
      "Efficient units:\n",
      "'Leroy Sané': 1.0000000000436755, 'Bernardo Silva': 1.5900071914830856, 'Romelu Lukaku': 3.552930927783648, 'Lucas': 5.199148905236307, 'Harry Kane': 1.0000000001497893, 'Mohamed Salah': 1.0000002575762057, 'Danny Ings': 7.891175207681022, 'Josh Sims': 13.401591215005622, 'Riyad Mahrez': 1.124052421633353, 'Christian Benteke': 3.0413299044330437, 'Daniel Sturridge': 9.464699037349193, 'Sam Vokes': 3.976400816548532, 'Tomer Hemed': 5.540486312576086\n",
      "\n",
      "\n",
      "Inefficient units:\n",
      "'Raheem Sterling': 0.8860157504907372, 'Sergio Agüero': 0.990629679735921, 'Gabriel Jesus': 0.6185686543520579, 'Alexis Sánchez': 0.5698035272020328, 'Anthony Martial': 0.8356263213416812, 'Jesse Lingard': 0.6821794211479711, 'Marcus Rashford': 0.6826264737515373, 'Zlatan Ibrahimovic': 0.4922843443884874, 'Heung-Min Son': 0.7231103038655945, 'Érik Lamela': 0.6776197089005593, 'Fernando Llorente': 0.4933603121682915, 'Eden Hazard': 0.8037364904016352, 'Pedro': 0.40456154203138417, 'Willian': 0.7355765626354404, 'Álvaro Morata': 0.5385675970074294, 'Olivier Giroud': 0.5282342968346834, 'Sadio Mané': 0.6977375189597136, 'Ben Woodburn': 0.6745689693820012, 'Dominic Solanke': 0.15304853737529286, 'Alex Iwobi': 0.4437275756501528, 'Pierre-Emerick Aubameyang': 0.9162127014998505, 'Alexandre Lacazette': 0.4779646855101876, 'Danny Welbeck': 0.29466293709178976, 'Yannick Bolasie': -0.0, 'Theo Walcott': 0.737531212331295, 'Cenk Tosun': 0.7240181018714383, 'Dominic Calvert-Lewin': 0.4811589777566421, 'Oumar Niasse': 0.9534418594150132, 'Nathan Redmond': 0.765327085457235, 'Dusan Tadic': 0.2769748548119356, 'Manolo Gabbiadini': 0.3540659940349833, 'Charlie Austin': 0.81828577396445, 'Shane Long': 0.23049124135648472, 'Guido Carrillo': 0.6206030540367182, 'Marko Arnautovic': -4.2666053255789286e-05, 'Michail Antonio': 0.20812101894422477, 'Chicharito': 0.3764707339982663, 'Andy Carroll': 0.2580974615173097, 'Jordan Hugill': 0.39577796697242795, 'Demarai Gray': 0.20807359333228428, 'Jamie Vardy': 0.6995839538863995, 'Kelechi Iheanacho': 0.6966683646375678, 'Shinji Okazaki': 0.5086362638475742, 'Fousseni Diabaté': 0.2192244820209358, 'Bakary Sako': 0.7692797524553168, 'Wilfried Zaha': 0.40886805683108346, 'Andros Townsend': 0.5870967753012742, 'Chung-Yong Lee': -0.0, 'Alexander Sörloth': 0.9241573262360382, 'Eric Maxim Choupo-Moting': 0.4333333425781364, 'Jesé': 0.4893516372985351, 'Ramadan Sobhi': 0.30759318003571934, 'Xherdan Shaqiri': 0.5752097972787827, 'Saido Berahino': 0.1831441253371344, 'Mame Diouf': 0.24021120443175992, 'Peter Crouch': 0.3198418106223112, 'Richarlison': 0.40882967608244974, 'Gerard Deulofeu': 0.5408123748219424, 'André Carrillo': 0.21109246800093356, 'Dodi Lukebakio': 0.5098800089061011, 'Andre Gray': 0.36572421453886966, 'Troy Deeney': 0.4501642932429256, 'Stefano Okaka': 0.3187652145800036, 'Jerome Sinclair': -0.0, 'Nacer Chadli': 0.2960038947042017, 'James McClean': 0.0795654427404867, 'Hal Robson-Kanu': 0.18356375048452236, 'Oliver Burke': 0.3679279100676915, 'Matt Phillips': 0.29293809860066944, 'Jonathan Leko': 0.4944063001361981, 'José Salomón Rondón': 0.3221144333565683, 'Jay Rodríguez': 0.3008206493529987, 'André Ayew': 0.4679449240996936, 'Wayne Routledge': 0.1842996341361579, 'Kenji Gorré': 0.7891016584380778, 'Luciano Narsingh': 0.556122877176301, 'Nathan Dyer': 0.20187975552954504, 'Wilfried Bony': 0.3375147309039379, 'Tammy Abraham': 0.5107656175520306, 'Jordan Ayew': 0.3562781196489725, 'Christian Atsu': 0.21054405064160112, 'Jacob Murphy': 0.1407137943526543, 'Islam Slimani': 0.7774726123155066, 'Dwight Gayle': 0.42631366172320306, 'Joselu': 0.24052399445174655, \"Georges-Kevin N'Koudou\": 0.4745213703891534, 'Aaron Lennon': 0.3961349349207346, 'Johann Berg Gudmundsson': 0.48750025099337885, 'Jonathan Walters': 0.0, 'Chris Wood': 0.5835336445848287, 'Nahki Wells': 0.0, 'José Izquierdo': 0.554143095373424, 'Jirí Skalák': -0.0, 'Anthony Knockaert': 0.29685636607463417, 'Jürgen Locadia': 0.9930215783444133, 'Leonardo Ulloa': 0.49426720609816516, 'Sam Baldock': 0.41304522016563033, 'Glenn Murray': 0.6807413347646241, 'Ryan Fraser': 0.36310344838049896, 'Junior Stanislas': 0.3747572960234425, 'Marc Pugh': 0.2660613269830555, 'Jordon Ibe': 0.7023065165694732, 'Callum Wilson': 0.496397723274667, 'Lys Mousset': 0.25538441103723863, 'Jermain Defoe': 0.2963212825757345, 'Tom Ince': 0.07352806951116084, 'Rajiv van La Parra': 0.22799767567824783, 'Sean Scannell': 0.14309778625276393, 'Steve Mounié': 0.4600439865400728, 'Laurent Depoitre': 0.2928379752209093, 'Elias Kachunga': 0.23854622509850237, 'Collin Quaner': 0.44375672180611603\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n",
      "[-0.04861527  0.02960498] [ 4.56953489 -0.02500932]\n"
     ]
    }
   ],
   "source": [
    "# inpt_arr_2 = np.array(inputss)\n",
    "# outpt_arr_2 = np.array(outputss)\n",
    "\n",
    "class DEA(object):\n",
    "    random.seed(5)\n",
    "    def __init__(self, inputs, outputs):\n",
    "        \"\"\"\n",
    "        Initialize the DEA object with input data\n",
    "        n = number of entities (observations)\n",
    "        m = number of inputs (variables, features)\n",
    "        r = number of outputs\n",
    "        :param inputs: inputs, n x m numpy array\n",
    "        :param outputs: outputs, n x r numpy array\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "\n",
    "        # supplied data\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "\n",
    "        # parameters\n",
    "        self.n = inputs.shape[0]\n",
    "        self.m = inputs.shape[1]\n",
    "        self.r = outputs.shape[1]\n",
    "\n",
    "        # iterators\n",
    "        self.unit_ = range(self.n)\n",
    "        self.input_ = range(self.m)\n",
    "        self.output_ = range(self.r)\n",
    "\n",
    "        # result arrays\n",
    "        self.output_w = np.zeros((self.r, 1), dtype=np.float)  # output weights\n",
    "        self.input_w = np.zeros((self.m, 1), dtype=np.float)  # input weights\n",
    "        self.lambdas = np.zeros((self.n, 1), dtype=np.float)  # unit efficiencies\n",
    "        self.efficiency = np.zeros_like(self.lambdas)  # thetas\n",
    "\n",
    "        # names\n",
    "        self.names = []\n",
    "\n",
    "    def __efficiency(self, unit):\n",
    "        \"\"\"\n",
    "        Efficiency function with already computed weights\n",
    "        :param unit: which unit to compute for\n",
    "        :return: efficiency\n",
    "        \"\"\"\n",
    "\n",
    "        # compute efficiency\n",
    "        denominator = np.dot(self.inputs, self.input_w)\n",
    "        numerator = np.dot(self.outputs, self.output_w)\n",
    "\n",
    "        return (numerator/denominator)[unit]\n",
    "\n",
    "    def __target(self, x, unit):\n",
    "        \"\"\"\n",
    "        Theta target function for one unit\n",
    "        :param x: combined weights\n",
    "        :param unit: which production unit to compute\n",
    "        :return: theta\n",
    "        \"\"\"\n",
    "        in_w, out_w, lambdas = x[:self.m], x[self.m:(self.m+self.r)], x[(self.m+self.r):]  # unroll the weights\n",
    "        denominator = np.dot(self.inputs[unit], in_w)\n",
    "        numerator = np.dot(self.outputs[unit], out_w)\n",
    "\n",
    "        return numerator/denominator\n",
    "\n",
    "    def __constraints(self, x, unit):\n",
    "        \"\"\"\n",
    "        Constraints for optimization for one unit\n",
    "        :param x: combined weights\n",
    "        :param unit: which production unit to compute\n",
    "        :return: array of constraints\n",
    "        \"\"\"\n",
    "\n",
    "        in_w, out_w, lambdas = x[:self.m], x[self.m:(self.m+self.r)], x[(self.m+self.r):]  # unroll the weights\n",
    "        constr = []  # init the constraint array\n",
    "\n",
    "        # for each input, lambdas with inputs\n",
    "        for input in self.input_:\n",
    "            t = self.__target(x, unit)\n",
    "            lhs = np.dot(self.inputs[:, input], lambdas)\n",
    "            cons = t*self.inputs[unit, input] - lhs\n",
    "            constr.append(cons)\n",
    "\n",
    "        # for each output, lambdas with outputs\n",
    "        for output in self.output_:\n",
    "            lhs = np.dot(self.outputs[:, output], lambdas)\n",
    "            cons = lhs - self.outputs[unit, output]\n",
    "            constr.append(cons)\n",
    "\n",
    "        # for each unit\n",
    "        for u in self.unit_:\n",
    "            constr.append(lambdas[u])\n",
    "\n",
    "        return np.array(constr)\n",
    "\n",
    "    def __optimize(self):\n",
    "        \"\"\"\n",
    "        Optimization of the DEA model\n",
    "        Use: http://docs.scipy.org/doc/scipy-0.17.0/reference/generated/scipy.optimize.linprog.html\n",
    "        A = coefficients in the constraints\n",
    "        b = rhs of constraints\n",
    "        c = coefficients of the target function\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        d0 = self.m + self.r + self.n\n",
    "        # iterate over units\n",
    "        for unit in self.unit_:\n",
    "            # weights\n",
    "            x0 = np.random.rand(d0) - 0.5\n",
    "            x0 = fmin_slsqp(self.__target, x0, f_ieqcons=self.__constraints, args=(unit,))\n",
    "            # unroll weights\n",
    "            self.input_w, self.output_w, self.lambdas = x0[:self.m], x0[self.m:(self.m+self.r)], x0[(self.m+self.r):]\n",
    "            self.efficiency[unit] = self.__efficiency(unit)\n",
    "\n",
    "    def name_units(self, names):\n",
    "        \"\"\"\n",
    "        Provide names for units for presentation purposes\n",
    "        :param names: a list of names, equal in length to the number of units\n",
    "        :return: nothing\n",
    "        \"\"\"\n",
    "\n",
    "        assert(self.n == len(names))\n",
    "\n",
    "        self.names = names\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Optimize the dataset, generate basic table\n",
    "        :return: table\n",
    "        \"\"\"\n",
    "\n",
    "        self.__optimize()  # optimize\n",
    "        print(\"---------------------------\\n\")       \n",
    "\n",
    "        m = {}\n",
    "        l = {}         \n",
    "        for n, eff in enumerate(self.efficiency):           \n",
    "            if eff >= 1.:\n",
    "                m.update({self.names[n]: eff[0]}) \n",
    "            else:\n",
    "                l.update({self.names[n]: eff[0]})                 \n",
    "        print(\"Efficient units:\")     \n",
    "        print(str(m).replace(\"{\",\"\").replace(\"}\", \"\"))       \n",
    "        print(\"\\n\")    \n",
    "        print(\"Inefficient units:\")\n",
    "        print(str(l).replace(\"{\",\"\").replace(\"}\", \"\"))  \n",
    "        for n in enumerate(self.efficiency):\n",
    "            print(self.input_w, self.output_w)\n",
    "        \n",
    "\n",
    "\n",
    "dea = DEA(inpt_arr_2, outpt_arr_2)\n",
    "dea.name_units(comp_2)\n",
    "dea.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.00000003588\n",
      "            Iterations: 18\n",
      "            Function evaluations: 650\n",
      "            Gradient evaluations: 18\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.966316603098\n",
      "            Iterations: 14\n",
      "            Function evaluations: 504\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.99581214551\n",
      "            Iterations: 20\n",
      "            Function evaluations: 722\n",
      "            Gradient evaluations: 20\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.932140293891\n",
      "            Iterations: 15\n",
      "            Function evaluations: 540\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.00000000444\n",
      "            Iterations: 8\n",
      "            Function evaluations: 288\n",
      "            Gradient evaluations: 8\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.928787206476\n",
      "            Iterations: 9\n",
      "            Function evaluations: 324\n",
      "            Gradient evaluations: 9\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.922897920306\n",
      "            Iterations: 12\n",
      "            Function evaluations: 432\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.00000001492\n",
      "            Iterations: 10\n",
      "            Function evaluations: 360\n",
      "            Gradient evaluations: 10\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.871090264284\n",
      "            Iterations: 18\n",
      "            Function evaluations: 648\n",
      "            Gradient evaluations: 18\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.00000015567\n",
      "            Iterations: 17\n",
      "            Function evaluations: 612\n",
      "            Gradient evaluations: 17\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.0000000317\n",
      "            Iterations: 14\n",
      "            Function evaluations: 504\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.974436819017\n",
      "            Iterations: 14\n",
      "            Function evaluations: 504\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.873789625393\n",
      "            Iterations: 14\n",
      "            Function evaluations: 505\n",
      "            Gradient evaluations: 14\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.984484190762\n",
      "            Iterations: 9\n",
      "            Function evaluations: 324\n",
      "            Gradient evaluations: 9\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.999999999517\n",
      "            Iterations: 6\n",
      "            Function evaluations: 216\n",
      "            Gradient evaluations: 6\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.00000000023\n",
      "            Iterations: 12\n",
      "            Function evaluations: 432\n",
      "            Gradient evaluations: 12\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.00000000005\n",
      "            Iterations: 6\n",
      "            Function evaluations: 217\n",
      "            Gradient evaluations: 6\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.0\n",
      "            Iterations: 9\n",
      "            Function evaluations: 325\n",
      "            Gradient evaluations: 9\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.982341644768\n",
      "            Iterations: 18\n",
      "            Function evaluations: 648\n",
      "            Gradient evaluations: 18\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.00000000082\n",
      "            Iterations: 7\n",
      "            Function evaluations: 252\n",
      "            Gradient evaluations: 7\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.00000000023\n",
      "            Iterations: 10\n",
      "            Function evaluations: 360\n",
      "            Gradient evaluations: 10\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 1.00000042344\n",
      "            Iterations: 7\n",
      "            Function evaluations: 252\n",
      "            Gradient evaluations: 7\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.982558049119\n",
      "            Iterations: 22\n",
      "            Function evaluations: 793\n",
      "            Gradient evaluations: 22\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.999999999995\n",
      "            Iterations: 6\n",
      "            Function evaluations: 217\n",
      "            Gradient evaluations: 6\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.984999924808\n",
      "            Iterations: 15\n",
      "            Function evaluations: 540\n",
      "            Gradient evaluations: 15\n",
      "Optimization terminated successfully.    (Exit mode 0)\n",
      "            Current function value: 0.956899154838\n",
      "            Iterations: 18\n",
      "            Function evaluations: 649\n",
      "            Gradient evaluations: 18\n",
      "---------------------------\n",
      "\n",
      "Efficient units:\n",
      "'A': 1.0000000358829255, 'E': 1.0000000044424062, 'H': 1.0000000149214712, 'K': 1.0000000316962117, 'J': 1.0000001556692342, 'Q': 1.0000000000484728, 'P': 1.0000000002282767, 'R': 1.0000000000013682, 'U': 1.0000000002335174, 'T': 1.0000000008183176, 'V': 1.0000004234448105\n",
      "\n",
      "\n",
      "Inefficient units:\n",
      "'C': 0.9958121455099681, 'B': 0.9663166030979251, 'D': 0.9321402938915002, 'G': 0.9228979203055871, 'F': 0.9287872064756271, 'I': 0.8710902642835067, 'M': 0.8737896253930998, 'L': 0.9744368190168068, 'O': 0.9999999995173697, 'N': 0.9844841907617617, 'S': 0.9823416447678736, 'W': 0.9825580491186346, 'Y': 0.9849999248084184, 'X': 0.9999999999952303, 'Z': 0.9568991548383973\n"
     ]
    }
   ],
   "source": [
    "#Calculating DEA\n",
    "class DEA(object):\n",
    "\n",
    "    def __init__(self, inputs, outputs):\n",
    "        \"\"\"\n",
    "        Initialize the DEA object with input data\n",
    "        n = number of entities (observations)\n",
    "        m = number of inputs (variables, features)\n",
    "        r = number of outputs\n",
    "        :param inputs: inputs, n x m numpy array\n",
    "        :param outputs: outputs, n x r numpy array\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "\n",
    "        # supplied data\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "\n",
    "        # parameters\n",
    "        self.n = inputs.shape[0]\n",
    "        self.m = inputs.shape[1]\n",
    "        self.r = outputs.shape[1]\n",
    "\n",
    "        # iterators\n",
    "        self.unit_ = range(self.n)\n",
    "        self.input_ = range(self.m)\n",
    "        self.output_ = range(self.r)\n",
    "\n",
    "        # result arrays\n",
    "        self.output_w = np.zeros((self.r, 1), dtype=np.float)  # output weights\n",
    "        self.input_w = np.zeros((self.m, 1), dtype=np.float)  # input weights\n",
    "        self.lambdas = np.zeros((self.n, 1), dtype=np.float)  # unit efficiencies\n",
    "        self.efficiency = np.zeros_like(self.lambdas)  # thetas\n",
    "\n",
    "        # names\n",
    "        self.names = []\n",
    "\n",
    "    def __efficiency(self, unit):\n",
    "        \"\"\"\n",
    "        Efficiency function with already computed weights\n",
    "        :param unit: which unit to compute for\n",
    "        :return: efficiency\n",
    "        \"\"\"\n",
    "\n",
    "        # compute efficiency\n",
    "        denominator = np.dot(self.inputs, self.input_w)\n",
    "        numerator = np.dot(self.outputs, self.output_w)\n",
    "\n",
    "        return (numerator/denominator)[unit]\n",
    "\n",
    "    def __target(self, x, unit):\n",
    "        \"\"\"\n",
    "        Theta target function for one unit\n",
    "        :param x: combined weights\n",
    "        :param unit: which production unit to compute\n",
    "        :return: theta\n",
    "        \"\"\"\n",
    "        in_w, out_w, lambdas = x[:self.m], x[self.m:(self.m+self.r)], x[(self.m+self.r):]  # unroll the weights\n",
    "        denominator = np.dot(self.inputs[unit], in_w)\n",
    "        numerator = np.dot(self.outputs[unit], out_w)\n",
    "\n",
    "        return numerator/denominator\n",
    "\n",
    "    def __constraints(self, x, unit):\n",
    "        \"\"\"\n",
    "        Constraints for optimization for one unit\n",
    "        :param x: combined weights\n",
    "        :param unit: which production unit to compute\n",
    "        :return: array of constraints\n",
    "        \"\"\"\n",
    "\n",
    "        in_w, out_w, lambdas = x[:self.m], x[self.m:(self.m+self.r)], x[(self.m+self.r):]  # unroll the weights\n",
    "        constr = []  # init the constraint array\n",
    "\n",
    "        # for each input, lambdas with inputs\n",
    "        for input in self.input_:\n",
    "            t = self.__target(x, unit)\n",
    "            lhs = np.dot(self.inputs[:, input], lambdas)\n",
    "            cons = t*self.inputs[unit, input] - lhs\n",
    "            constr.append(cons)\n",
    "\n",
    "        # for each output, lambdas with outputs\n",
    "        for output in self.output_:\n",
    "            lhs = np.dot(self.outputs[:, output], lambdas)\n",
    "            cons = lhs - self.outputs[unit, output]\n",
    "            constr.append(cons)\n",
    "\n",
    "        # for each unit\n",
    "        for u in self.unit_:\n",
    "            constr.append(lambdas[u])\n",
    "\n",
    "        return np.array(constr)\n",
    "\n",
    "    def __optimize(self):\n",
    "        \"\"\"\n",
    "        Optimization of the DEA model\n",
    "        Use: http://docs.scipy.org/doc/scipy-0.17.0/reference/generated/scipy.optimize.linprog.html\n",
    "        A = coefficients in the constraints\n",
    "        b = rhs of constraints\n",
    "        c = coefficients of the target function\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        d0 = self.m + self.r + self.n\n",
    "        # iterate over units\n",
    "        for unit in self.unit_:\n",
    "            # weights\n",
    "            x0 = np.random.rand(d0) - 0.5\n",
    "            x0 = fmin_slsqp(self.__target, x0, f_ieqcons=self.__constraints, args=(unit,))\n",
    "            # unroll weights\n",
    "            self.input_w, self.output_w, self.lambdas = x0[:self.m], x0[self.m:(self.m+self.r)], x0[(self.m+self.r):]\n",
    "            self.efficiency[unit] = self.__efficiency(unit)\n",
    "\n",
    "    def name_units(self, names):\n",
    "        \"\"\"\n",
    "        Provide names for units for presentation purposes\n",
    "        :param names: a list of names, equal in length to the number of units\n",
    "        :return: nothing\n",
    "        \"\"\"\n",
    "\n",
    "        assert(self.n == len(names))\n",
    "\n",
    "        self.names = names\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Optimize the dataset, generate basic table\n",
    "        :return: table\n",
    "        \"\"\"\n",
    "\n",
    "        self.__optimize()  # optimize\n",
    "        print(\"---------------------------\\n\")       \n",
    "\n",
    "        m = {}\n",
    "        l = {}         \n",
    "        for n, eff in enumerate(self.efficiency):           \n",
    "            if eff >= 1.:\n",
    "                m.update({self.names[n]: eff[0]}) \n",
    "            else:\n",
    "                l.update({self.names[n]: eff[0]})                 \n",
    "        print(\"Efficient units:\")     \n",
    "        print(str(m).replace(\"{\",\"\").replace(\"}\", \"\"))       \n",
    "        print(\"\\n\")    \n",
    "        print(\"Inefficient units:\")\n",
    "        print(str(l).replace(\"{\",\"\").replace(\"}\", \"\"))         \n",
    "            \n",
    "\n",
    "dea = DEA(inpt_arr,outpt_arr)\n",
    "dea.name_units(comp)\n",
    "dea.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculation efficiency by dividing output by input to be used for the graph\n",
    "frst = outpt_df.div(inpt_df.x1, axis = 0)\n",
    "frst.columns = ['y1-x1', 'y2-x1', 'y3-x1']\n",
    "sec = outpt_df.div(inpt_df.x2, axis = 0)\n",
    "sec.columns = ['y1-x2', 'y2-x2', 'y3-x2']\n",
    "th = outpt_df.div(inpt_df.x3, axis = 0)\n",
    "th.columns = ['y1-x3', 'y2-x3', 'y3-x3']\n",
    "fo = outpt_df.div(inpt_df.x4, axis = 0)\n",
    "fo.columns = ['y1-x4', 'y2-x4', 'y3-x4']\n",
    "fi = outpt_df.div(inpt_df.x5, axis = 0)\n",
    "fi.columns = ['y1-x5', 'y2-x5', 'y3-x5']\n",
    "complete = pd.DataFrame(df.iloc[:,0]).join(frst).join(sec).join(th).join(fo).join(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building a graph\n",
    "eff = ['y1-x1', 'y2-x1', 'y3-x1', 'y1-x2', 'y2-x2', 'y3-x2',\n",
    "       'y1-x3', 'y2-x3', 'y3-x3', 'y1-x4', 'y2-x4', 'y3-x4', 'y1-x5',\n",
    "       'y2-x5', 'y3-x5']\n",
    "x = [i for i, _ in enumerate(eff)]\n",
    "\n",
    "min_max_range = {}\n",
    "for e in eff:\n",
    "    min_max_range[e] = [complete[e].min(), complete[e].max(), np.ptp(complete[e])]\n",
    "    complete[e] = np.true_divide(complete[e] - complete[e].min(), np.ptp(complete[e]))\n",
    "    \n",
    "fig, axes = plt.subplots(1, len(x)-1, sharey = False, figsize = (50,10), gridspec_kw = {'wspace':0, 'hspace':0})    \n",
    "\n",
    "# Plot each row\n",
    "for i, ax in enumerate(axes):\n",
    "    for idx in complete.index:\n",
    "        ax.plot(x, complete.loc[idx, eff], color='gray')  \n",
    "    ax.set_xlim([x[i], x[i+1]])\n",
    "\n",
    "plt.title(\"Efficiency by a firm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "firms = ['1', '2', '3']\n",
    "df = pd.DataFrame(firms)\n",
    "output = { 'firms': ['1', '2', '3'],\n",
    "'Sales': [150, 200, 50],\n",
    "'Profit':[200, 210, 90],\n",
    "'Sth':[1, 2, 3]}\n",
    "df1 = pd.DataFrame.from_dict(output)\n",
    "inputs = { 'firms': ['1', '2', '3'],\n",
    "'Salary': [10000, 20000, 500],\n",
    "'employees':[2, 4, 5]}\n",
    "df2 = pd.DataFrame.from_dict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete.to_csv('./files/output.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
